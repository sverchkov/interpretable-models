{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning trees to explain models\n",
    "\n",
    "_Yuriy Sverchkov_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanation\n",
    "\n",
    "* Highly accurate supervised learning models are often difficult to interpret\n",
    "    * Deep networks\n",
    "    * Random forests\n",
    "    * Boosted models\n",
    "    * Nonlinear SVMs\n",
    "* There is a need in various settings to interpret model decisions\n",
    "    * High-stakes decision making\n",
    "        * Medical\n",
    "        * Financial\n",
    "        * Legal\n",
    "    * Legal protections\n",
    "    * User trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc model-agnostic model translation\n",
    "\n",
    "* __Post-hoc__: given a learned model $f: \\mathcal X \\leftarrow \\mathcal Y$\n",
    "* __model-agnostic__: without assumptions about the inner workings of the model\n",
    "    * Contrast with saliency maps for CNNs\n",
    "* __model translation__: we learn a model $g$ that performs like $f$ and is interpretable\n",
    "    * Also called mimic learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees: our interpretable model of choice\n",
    "\n",
    "* Pros:\n",
    "    * Encode decision logic transparently\n",
    "    * Cover the entire feature space by design\n",
    "* Cons:\n",
    "    * Relatively poor classifiers/regressors\n",
    "    * Accurate trees tend to be deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a decision tree\n",
    "\n",
    "* Internal nodes represent conditions on features\n",
    "    * Commonly used conditions:\n",
    "        * Thresholds for continuous/ordinal features\n",
    "        * One-or-rest for discrete features\n",
    "        * Every-value split for discrete features\n",
    "    * Less-commonly used conditions:\n",
    "        * $m$-of-$n$ conditions\n",
    "    * Others?\n",
    "        * Interval segmentation\n",
    "        * Linear functions of multiple features\n",
    "        * Latent/derived features\n",
    "* Leaf nodes represent decisions\n",
    "    * Fixed-value prediction\n",
    "    * Fixed distribution\n",
    "    * Simple model ('model trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree learning (from training data)\n",
    "\n",
    "* Standard: greedyly grow the tree\n",
    "    * Scores: gini, entropy\n",
    "    * Algorithm:\n",
    "        * _At each potential internal node, select a split that maximizes the score on the training data._\n",
    "        * _Stop splitting according to some criteria._\n",
    "            * Tree depth\n",
    "            * Data scarecity\n",
    "            * Degenerate score\n",
    "    * Problems\n",
    "        * Maximizing the score at a higher node means possibly suboptimal choices lower down\n",
    "        * Training data at each decision dwindles as the tree grows\n",
    "* Other work:\n",
    "    * Bayesian\n",
    "    * Other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation tree learning\n",
    "\n",
    "* Given a model $f: \\mathcal X \\leftarrow \\mathcal Y$ learn a decision tree with high fidelity to $f$\n",
    "* Following a similar algorithm to standard decision tree learning:\n",
    "    * Given a score function\n",
    "    * Algorithm:\n",
    "        * __At each potential internal node, select a split that maximizes the score on data $(X, f(X))$\n",
    "            * Given a generator for $X$, we solve the data scarecity issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions in learning an explaining decision tree\n",
    "\n",
    "* Condition classes at internal nodes\n",
    "* Classes of leaf nodes\n",
    "* Space search strategy\n",
    "* Local score function\n",
    "* Unlabeled data generator\n",
    "* Stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `generalizedtrees` python package\n",
    "\n",
    "* [link]\n",
    "* Python package that implements a joint framework for all variants of tree learning and allows swapping in different components that correspond to each decision.\n",
    "* Current status\n",
    "    * Implemented standard decision tree learning (verified against scikit-learn implementation)\n",
    "    * Implemented basic explanation tree learning\n",
    "    * Ongoing: re-imlementation of Trepan (Craven 1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary evaluation\n",
    "\n",
    "* Evaluation on asthma dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future planned evaluation\n",
    "\n",
    "Sweeping comparison of many variants of tree learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bitvenvvenv913e33813d9e44b89d250b98ae9a5f2c",
   "display_name": "Python 3.6.5 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}