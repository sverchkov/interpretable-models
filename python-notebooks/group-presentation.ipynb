{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "# Learning trees to explain models\n",
    "\n",
    "_Group Meeting_\n",
    "\n",
    "_May 8, 2020_\n",
    "\n",
    "_Yuriy Sverchkov_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Model explanation\n",
    "\n",
    "* Highly accurate supervised learning models are often difficult to interpret\n",
    "    * Deep networks\n",
    "    * Random forests\n",
    "    * Boosted models\n",
    "    * Nonlinear SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "fragment"
    }},
   "source": [
    "* There is a need in various settings to interpret model decisions\n",
    "    * High-stakes decision making\n",
    "        * Medical\n",
    "        * Financial\n",
    "        * Legal\n",
    "    * Legal protections\n",
    "    * User trust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Post-hoc model-agnostic model translation\n",
    "\n",
    "* __Post-hoc__: given a learned model $f: \\mathcal X \\rightarrow \\mathcal Y$\n",
    "* __model-agnostic__: without assumptions about the inner workings of the model\n",
    "    * Contrast with saliency maps for CNNs\n",
    "* __model translation__: we learn a model $g$ that performs like $f$ and is interpretable\n",
    "    * Also called mimic learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Decision trees: our interpretable model of choice\n",
    "\n",
    "* Pros:\n",
    "    * Encode decision logic transparently\n",
    "    * Cover the entire feature space by design\n",
    "* Cons:\n",
    "    * Relatively poor classifiers/regressors\n",
    "    * Accurate trees tend to be deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Anatomy of a decision tree: internal nodes\n",
    "\n",
    "Internal nodes represent conditions on features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "fragment"
    }},
   "source": [
    "* Axis-aligned splits (most common):\n",
    "    * Thresholds for continuous/ordinal features\n",
    "    * One-or-rest for discrete features\n",
    "    * Every-value split for discrete features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "fragment"
    }},
   "source": [
    "* Composite condition splits:\n",
    "    * $m$-of-$n$ conditions\n",
    "    * Linear function splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "fragment"
    }},
   "source": [
    "* Future ideas:\n",
    "    * Interval segmentation\n",
    "    * Latent/derived features\n",
    "    * Splits for temporal data (e.g. was $x_i > \\theta$ at some $t < \\tau$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Anatomy of a decision tree: leaf nodes\n",
    "\n",
    "Leaf nodes represent decisions\n",
    "\n",
    "* Fixed-value prediction\n",
    "* Fixed distribution\n",
    "* Simple model ('model trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Decision tree learning (from training data)\n",
    "\n",
    "* Standard: greedily grow the tree\n",
    "    * Scores: gini, entropy, variance, Bayesian\n",
    "    * Algorithm:\n",
    "        * _At each potential internal node, select a split that maximizes the score on the training data._\n",
    "        * _Stop splitting according to some criteria._\n",
    "            * Tree depth\n",
    "            * Data scarecity\n",
    "            * Degenerate score\n",
    "    * Problems\n",
    "        * Maximizing the score at a higher node means possibly suboptimal choices lower down\n",
    "        * Training data at each decision dwindles as the tree grows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Explanation tree learning\n",
    "\n",
    "* Given a model $f: \\mathcal X \\rightarrow \\mathcal Y$ learn a decision tree with high fidelity to $f$\n",
    "* Following a similar algorithm to standard decision tree learning:\n",
    "    * Given a score function\n",
    "    * Algorithm:\n",
    "        * __At each potential internal node, select a split that maximizes the score on data $(X, f(X))$__\n",
    "            * Given a generator for $X$, we solve the data scarecity issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "fragment"
    }},
   "source": [
    "* Other approaches:\n",
    "    * Frosst and Hinton 2017: Learn splits and leaves by gradient descent for a fixed tree skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Decisions in learning an explaining decision tree\n",
    "\n",
    "* Condition classes at internal nodes\n",
    "* Classes of leaf nodes\n",
    "* Space search strategy\n",
    "* Local score function\n",
    "* Unlabeled data generator\n",
    "* Stopping criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## The `generalizedtrees` python package\n",
    "\n",
    "* [https://github.com/Craven-Biostat-Lab/generalizedtrees]\n",
    "* Python package that implements a joint framework for all variants of tree learning and allows swapping in different components that correspond to each design decision.\n",
    "* Already implemented:\n",
    "    * Standard decision tree learning (verified against Scikit-Learn implementation)\n",
    "    * Basic model-tree learning\n",
    "    * Basic explanation (model translation) tree learning\n",
    "* Compatible with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### `generalizedtrees` details: explanation tree learner\n",
    "\n",
    "* Input and parameters:\n",
    "    * Black-box classifier\n",
    "    * Data generator\n",
    "    * Impurity score function\n",
    "    * Sample size to use for learning splits\n",
    "    * Depth limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"slideshow": {
        "slide_type": "fragment"
    }},
   "outputs": [],
   "source": [
    "Explainer = make_trepanlike_classifier(classifier, generator)\n",
    "ex_tree = Explainer(s_min, max_depth, score)\n",
    "ex_tree.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Planned evaluations\n",
    "\n",
    "* Datasets: Asthma exacerbations, MIMIC-III, UW-Health OMOP CDM extract, others\n",
    "* Sweeping comparison of many variants of explanation tree learning\n",
    "* Related past work\n",
    "    * Craven and Shavlik 1995\n",
    "    * Breiman and Shang 1996\n",
    "    * Bastani, Kim, and Bastani 2017\n",
    "    * Frosst and Hinton 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Trepan (Craven and Shavlik 1995)\n",
    "\n",
    "* Data generation: Independent per-feature kernel density or empirical distribution\n",
    "  * Distributions are re-estimated locally as the tree grows\n",
    "  * A statistical test is used to determine whether to re-estimate the distribution\n",
    "* Splits: $m$-of-$n$\n",
    "* Stopping criteria: Statistical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Born-again Trees (Breiman and Shang 1996)\n",
    "\n",
    "* Data generation: 'Smearing' - taking a training instance and randomly swapping a random subset of its features with other instances.\n",
    "* Data is generated first, then filtered by the tree\n",
    "  * Rejection rate informs node scores (nodes that less samples reach score lower)\n",
    "  * Accepted samples used to compute impurity at node\n",
    "* Stopping criteria: Exhaustion of original training data at a node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Bastani, Kim, and Bastani 2017\n",
    "\n",
    "* Data generation: mixture of gaussians.\n",
    "* Efficient sampling subject to constraints.\n",
    "* Only generated data is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "### Frosst and Hinton 2017\n",
    "\n",
    "* Internal nodes are logistic regression classifiers\n",
    "* Leaf nodes are softmax functions\n",
    "* Learning: tree structure is fixed, parameters (LR weights and biases, as well as softmax inputs) are learned by gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"slideshow": {
        "slide_type": "slide"
    }},
   "source": [
    "## Future developments\n",
    "\n",
    "* Using model trees for explanation\n",
    "* Splitting on higher-level concepts (feature groups, semantically meaningful latent features)\n",
    "* Trees for temporal data (e.g. $x_i > \\theta$ at $t < \\tau$)\n",
    "* Alternate scores for computing splits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bitvenvvenv913e33813d9e44b89d250b98ae9a5f2c",
   "display_name": "Python 3.6.5 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
