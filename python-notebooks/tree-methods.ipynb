{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing tree-learning methods\n",
    "\n",
    "There is a nontrivial variety of methods both about learning decision trees from data and learning trees from models.\n",
    "Here I catalogue various methods and outline their unique contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning trees from data\n",
    "TODO: CART, C4.5, Buntine 1991 - Bayesian trees\n",
    "Maybe: Kwok and Carter 1990, Heath et al. 1993."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning trees from models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREPAN\n",
    "*Craven and Shavlik 1995*\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Born again trees\n",
    "*Breiman and Shang 1996*\n",
    "\n",
    "#### Sample generation: \"smearing\"\n",
    "\n",
    "Given a training set $( \\mathbf x^{(i)} | i \\in 1:n )$ of $d$-dimensional instances,\n",
    "to generate a new training sample $\\mathbf x^{(+)}$:\n",
    "\n",
    "1. Select random $a \\in 1:n$\n",
    "2. For each $j \\in 1:d$,\n",
    "    with probability $p_\\mathrm{alt}$\n",
    "    let $x^{(+)}_j \\leftarrow x^{(a)}_j$, otherwise\n",
    "    let $x^{(+)}_j \\leftarrow x^{(b)}_j$ for random $b \\in 1:n$.\n",
    "\n",
    "#### Tree construction\n",
    "\n",
    "When considering expansion of node $t$, new samples are generated until $N_s$ samples reach the node, and their labels are estimated by the oracle. Let the number of samples that needed to be generated be $N_g$.\n",
    "\n",
    "Let $p_t = \\frac{N_s}{N_g}$, and let $p_{y=j}$ be the proportion of instances of class $j$ in this sample.\n",
    "The cost of node $t$ is then: $$\\mathrm{cost}(t) =  (1-\\max_j( p(j) ) )p_t$$\n",
    "\n",
    "For regression $\\mathrm{cost}(t) = \\mathrm{var}(t)p_t$\n",
    "\n",
    "Find the best split at $t$ and if either child has no original training set instances, then $t$ is terminal.\n",
    "\n",
    "#### Pruning and subtree selection\n",
    "\n",
    "As in CART.\n",
    "\n",
    "#### Additional notes:\n",
    "\n",
    "Tried a different data generation approach where components of the new instance are selected from its $k$ nearest neighbors, did not better than smearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilling a Neural Network Into a Soft Decision Tree\n",
    "\n",
    "_Frosst and Hinton (Google Brain Team) 2017_ [http://arxiv.org/abs/1711.09784]\n",
    "\n",
    "#### Hierarchical Mixture of Bigots\n",
    "\n",
    "For inner node $i$: the probability of sending $\\mathbf x$ to the rightmost branch is\n",
    "$$ p_i( \\mathbf x ) = \\sigma( \\mathbf x \\mathbf w_i + \\beta_i ) $$\n",
    "here, $\\mathbf w_i$ is a 'learned filter' and $\\sigma$ is the expit function.\n",
    "\n",
    "For a leaf node $\\ell$, the probability of predicting class $k$ is\n",
    "$$ Q^\\ell_k = \\frac{ \\exp( \\phi^\\ell_k ) }{ \\sum_{k'} \\exp( \\phi^\\ell_{k'}) } $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}