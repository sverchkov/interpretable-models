{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Existing tree-learning methods\n",
    "\n",
    "There is a nontrivial variety of methods both about learning decision trees from data and learning trees from models.\n",
    "Here I catalogue various methods and outline their unique contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning trees from data\n",
    "TODO: CART, C4.5, Buntine 1991 - Bayesian trees\n",
    "Maybe: Kwok and Carter 1990, Heath et al. 1993."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning trees from models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREPAN\n",
    "*Craven and Shavlik 1995*\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Born again trees\n",
    "*Breiman and Shang 1996*\n",
    "\n",
    "#### Sample generation: \"smearing\"\n",
    "\n",
    "Given a training set $( \\mathbf x^{(i)} | i \\in 1:n )$ of $d$-dimensional instances,\n",
    "to generate a new training sample $\\mathbf x^{(+)}$:\n",
    "\n",
    "1. Select random $a \\in 1:n$\n",
    "2. For each $j \\in 1:d$,\n",
    "    with probability $p_\\mathrm{alt}$\n",
    "    let $x^{(+)}_j \\leftarrow x^{(a)}_j$, otherwise\n",
    "    let $x^{(+)}_j \\leftarrow x^{(b)}_j$ for random $b \\in 1:n$.\n",
    "\n",
    "#### Tree construction\n",
    "\n",
    "When considering expansion of node $t$, new samples are generated until $N_s$ samples reach the node, and their labels are estimated by the oracle. Let the number of samples that needed to be generated be $N_g$.\n",
    "\n",
    "Let $p_t = \\frac{N_s}{N_g}$, and let $p_{y=j}$ be the proportion of instances of class $j$ in this sample.\n",
    "The cost of node $t$ is then: $$\\mathrm{cost}(t) =  (1-\\max_j( p(j) ) )p_t$$\n",
    "\n",
    "For regression $\\mathrm{cost}(t) = \\mathrm{var}(t)p_t$\n",
    "\n",
    "Find the best split at $t$ and if either child has no original training set instances, then $t$ is terminal.\n",
    "\n",
    "#### Pruning and subtree selection\n",
    "\n",
    "As in CART.\n",
    "\n",
    "#### Additional notes:\n",
    "\n",
    "Tried a different data generation approach where components of the new instance are selected from its $k$ nearest neighbors, did not better than smearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distilling a Neural Network Into a Soft Decision Tree\n",
    "\n",
    "_Frosst and Hinton (Google Brain Team) 2017_ [http://arxiv.org/abs/1711.09784]\n",
    "\n",
    "#### Hierarchical Mixture of Bigots\n",
    "\n",
    "For inner node $i$: the probability of sending $\\mathbf x$ to the rightmost branch is\n",
    "$$ p_i( \\mathbf x ) = \\sigma( \\mathbf x \\mathbf w_i + \\beta_i ) $$\n",
    "here, $\\mathbf w_i$ is a 'learned filter' and $\\sigma$ is the expit function.\n",
    "\n",
    "For a leaf node $\\ell$, the probability of predicting class $k$ is\n",
    "$$ Q^\\ell_k = \\frac{ \\exp( \\phi^\\ell_k ) }{ \\sum_{k'} \\exp( \\phi^\\ell_{k'}) } $$\n",
    "\n",
    "* A more explainable prediction is the one obtained by taking the prediction of the leaf with the maximum path probability\n",
    "* A path-weighted average of leaves also makes sense, but the explanation is not as explainable.\n",
    "\n",
    "#### Loss function\n",
    "Let $T_k$ be the target distribution and P^\\ell( \\mathbf x) be the probability of arriving at leaf $\\ell$.\n",
    "Then the corss-entropy loss function is\n",
    "$$ L(\\mathbf x ) = -\\log \\left( \\sum_{\\ell \\in \\text{leaves}} P^\\ell( \\mathbf x) T_k \\log Q_k^\\ell \\right) $$\n",
    "\n",
    "* The tree is learned by first deciding on the structure (size) and then training parameters.\n",
    "* Note that the \"splits\" are now linear functions.\n",
    "* Additional regularization is needed to drive the tree to have balanced mass distributions.\n",
    "\n",
    "#### Important point\n",
    "The black-box model (neural net) is used to provide the soft target distribution $T$.\n",
    "There is no sample generation, but there is a weighted average over all training samples used in the training instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Blackbox Models via Model Extraction\n",
    "\n",
    "_Bastani, Kim, and Bastani 2017_ [http://arxiv.org/abs/1705.08504]\n",
    "\n",
    "#### Mathematical definitions\n",
    "\n",
    "* $x \\in \\mathcal X \\subseteq \\mathbb R^d$\n",
    "* Axis-aligned constraint: $C= (x_i \\leq t)$ where $i \\in [d] = \\{1, \\ldots, d\\}$ and $t \\in \\mathbb R$.\n",
    "* The feasible set of $C$ is $\\mathcal F(C) = \\{x \\in \\mathcal X | x \\text{ satisfies } C\\}$.\n",
    "* A decision tree $T$ is a binary tree\n",
    "    * An internal node $N = (N_L, N_R, C)$\n",
    "    * A leaf node $N = (y); y\\in \\mathcal Y$\n",
    "    * Root node $N_T$\n",
    "    * A leaf node $(y)$ as a function $N(x) = y$\n",
    "    * An internal node $(N_L, N_R, C)$ as a function\n",
    "    $N(\\mathbf x) =\n",
    "    \\begin{cases}\n",
    "        N_L(x) & \\text{if } x \\in \\mathcal F(C) \\\\\n",
    "        N_R(x) & \\text{Otherwise}\n",
    "    \\end{cases}$\n",
    "    * The decision tree as a function $T(x) = N_T(x)$\n",
    "    * $C_N$ is the conjunction of constraints along a path to $N$. Defined recursively:    \n",
    "        * $C_T = \\mathrm{True}$\n",
    "        * Given $N = (N_L, N_R, C)$, $C_{N_L} = C_N \\land C$ and $C_{N_R} = C_N \\land \\neg C$.\n",
    "\n",
    "* Given black box $f: \\mathcal X \\rightarrow \\mathcal Y$,\n",
    "  Classification performance is held out fidelity, i.e.\n",
    "  $$ \\frac{ 1 }{ | X_\\text{test} | } \\sum_{x \\in X_\\text{test} } \\mathbb I [ T(x) = f(x) ] $$\n",
    "\n",
    "#### Tree extractor\n",
    "\n",
    "**Overview:** First use $X_\\text{train}$ to estimate a distribution $\\mathcal P$ over $\\mathcal X$, then greedily learn $T$.\n",
    "\n",
    "**Input distribution:**\n",
    "Fit $X_\\text{train}$ to a mixture of axis-aligned gaussians\n",
    "\n",
    "* $\\phi \\in \\mathbb R^k$\n",
    "* $j \\sim \\mathrm{Categorical}(\\phi)$\n",
    "* $x \\sim \\mathcal N( \\mu_j, \\Sigma_j)$, $\\Sigma_j$ diagonal\n",
    "\n",
    "**Exact Greedy Decision Tree**\n",
    "\n",
    "* Predetermined size $k$.\n",
    "* Uses gini impurity to determine splits and decide on which node to split.\n",
    "* The exact \"infinite data\" version is using the theoretical Gini impurity with respect to the probability distribution $\\mathcal P$\n",
    "\n",
    "**Estimated Greedy Decision Tree**\n",
    "\n",
    "* Gini impurity now estimated by sampling from $\\mathcal P$.\n",
    "* They show how to sample from the mixture of axis-aligned gaussians with respect to axis-aligned constraints.\n",
    "* They provide theoretical guarantees."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}